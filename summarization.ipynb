{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Abstract Summarization - Project Code\n",
    "\n",
    "Notebook for CSC791 project exploring extractive vs. abstractive summarization of arXiv abstracts. The dataset expected here is `~/arxiv-metadata.csv` with columns `title`, `summary`, and `category`. Cells are tuned for modest hardware; adjust sampling/config to scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook map\n",
    "- Imports and configuration\n",
    "- Data loading and lightweight preprocessing\n",
    "- Extractive baselines (Lead-k, TF-IDF sentence scoring)\n",
    "- Optional abstractive hook (only if transformers are available locally)\n",
    "- Evaluation helpers (ROUGE-n, ROUGE-L, embedding cosine)\n",
    "- Small sample run to verify the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('HF_HUB_OFFLINE', '1')\n",
    "os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Optional dependencies, the notebook runs without them.\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    SentenceTransformer = None\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "except ImportError:\n",
    "    pipeline = None\n",
    "    AutoTokenizer = None\n",
    "    AutoModelForSeq2SeqLM = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and runtime knobs\n",
    "DATA_PATH = Path(\"arxiv-metadata.csv\")  # adjust if the dataset lives elsewhere\n",
    "MODEL_CACHE_DIR = Path(\"hf_models\")\n",
    "EMBED_MODEL_PATH = MODEL_CACHE_DIR / \"sentence-transformers_all-MiniLM-L6-v2\"\n",
    "ABSTRACTIVE_MODEL_NAME = str(MODEL_CACHE_DIR / \"sshleifer_distilbart-cnn-12-6\")\n",
    "MAX_ROWS = 20000  # upper bound read from disk for quick iteration\n",
    "SAMPLE_SIZE = 5000  # random subset taken from MAX_ROWS to keep runtime low\n",
    "PER_CATEGORY_LIMIT = None  # e.g., 800 to cap per-category counts; None keeps all\n",
    "FOCUS_PREFIXES: List[str] = []  # e.g., ['cs.', 'math.'] to filter categories; empty means all\n",
    "MIN_WORDS = 30  # filter out abstracts shorter than this\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Evaluation batch size\n",
    "EVAL_ROWS = 40  # number of abstracts to score when smoke-testing methods\n",
    "RESULTS_PATH = Path(\"eval_results_sample.csv\")\n",
    "\n",
    "# Flags for optional components\n",
    "USE_SENTENCE_EMBEDDINGS = True\n",
    "USE_ABSTRACTIVE = True\n",
    "\n",
    "os.environ.setdefault(\"HF_HOME\", str(MODEL_CACHE_DIR.resolve()))\n",
    "\n",
    "print(f\"Using dataset at: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and environment\n",
    "Sets offline flags, imports dependencies, and pulls in optional transformer utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    text = normalize_whitespace(text)\n",
    "    # Simple sentence splitter, could be replaced by spaCy or NLTK later.\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [s for s in sentences if s]\n",
    "\n",
    "def load_arxiv_dataset(\n",
    "    path: Path = DATA_PATH,\n",
    "    max_rows: int = MAX_ROWS,\n",
    "    sample_size: int = SAMPLE_SIZE,\n",
    "    min_words: int = MIN_WORDS,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "    focus_prefixes: Optional[List[str]] = None,\n",
    "    per_category_limit: Optional[int] = PER_CATEGORY_LIMIT,\n",
    ") -> pd.DataFrame:\n",
    "    cols = [\"id\", \"title\", \"category\", \"summary\", \"summary_word_count\"]\n",
    "    df = pd.read_csv(path, usecols=cols, nrows=max_rows)\n",
    "    df[\"summary\"] = df[\"summary\"].apply(normalize_whitespace)\n",
    "    df = df[df[\"summary\"].str.split().str.len() >= min_words]\n",
    "    df = df.dropna(subset=[\"category\"])\n",
    "    if focus_prefixes:\n",
    "        df = df[df[\"category\"].str.startswith(tuple(focus_prefixes))]\n",
    "    if per_category_limit:\n",
    "        df = df.groupby(\"category\").head(per_category_limit).reset_index(drop=True)\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df = df.sample(sample_size, random_state=random_state)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Extractive summarizers\n",
    "\n",
    "def lead_k_summary(text: str, k: int = 1) -> str:\n",
    "    sentences = split_sentences(text)\n",
    "    return \" \".join(sentences[:k]) if sentences else \"\"\n",
    "\n",
    "def fit_tfidf_vectorizer(texts: Iterable[str], max_features: int = 8000) -> TfidfVectorizer:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "    vectorizer.fit(texts)\n",
    "    return vectorizer\n",
    "\n",
    "def tfidf_extractive_summary(\n",
    "    text: str,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    max_sentences: int = 2,\n",
    ") -> str:\n",
    "    sentences = split_sentences(text)\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    doc_vec = vectorizer.transform([text])\n",
    "    sent_vecs = vectorizer.transform(sentences)\n",
    "    scores = cosine_similarity(sent_vecs, doc_vec)[:, 0]\n",
    "    ranked_idx = np.argsort(scores)[::-1][:max_sentences]\n",
    "    # Preserve original order for readability\n",
    "    selected = [sentences[i] for i in sorted(ranked_idx)]\n",
    "    return \" \".join(selected)\n",
    "\n",
    "def textrank_extractive_summary(\n",
    "    text: str,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    max_sentences: int = 2,\n",
    "    damping: float = 0.85,\n",
    "    max_iter: int = 40,\n",
    "    tol: float = 1e-4,\n",
    ") -> str:\n",
    "    sentences = split_sentences(text)\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return \" \".join(sentences)\n",
    "    sent_vecs = vectorizer.transform(sentences)\n",
    "    sim = cosine_similarity(sent_vecs)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    row_sum = sim.sum(axis=1, keepdims=True)\n",
    "    norm_sim = np.divide(sim, row_sum, out=np.zeros_like(sim), where=row_sum != 0)\n",
    "    scores = np.ones(len(sentences)) / len(sentences)\n",
    "    for _ in range(max_iter):\n",
    "        prev = scores.copy()\n",
    "        scores = (1 - damping) / len(sentences) + damping * norm_sim.T.dot(scores)\n",
    "        if np.linalg.norm(scores - prev, 1) < tol:\n",
    "            break\n",
    "    ranked_idx = np.argsort(scores)[::-1][:max_sentences]\n",
    "    selected = [sentences[i] for i in sorted(ranked_idx)]\n",
    "    return \" \".join(selected)\n",
    "\n",
    "# Optional abstractive summarizer hook\n",
    "\n",
    "def get_abstractive_summarizer(model_name: str = \"facebook/bart-large-cnn\"):\n",
    "    if pipeline is None or AutoTokenizer is None or AutoModelForSeq2SeqLM is None:\n",
    "        return None\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "        mod = AutoModelForSeq2SeqLM.from_pretrained(model_name, local_files_only=True)\n",
    "        return pipeline(\"summarization\", model=mod, tokenizer=tok)\n",
    "    except Exception as exc:  # network or missing weights\n",
    "        print(f\"Abstractive model unavailable: {exc}\")\n",
    "        return None\n",
    "\n",
    "def abstractive_summary(\n",
    "    text: str,\n",
    "    summarizer,\n",
    "    min_length: int = 20,\n",
    "    max_length: int = 60,\n",
    ") -> str:\n",
    "    if summarizer is None:\n",
    "        return \"\"\n",
    "    result = summarizer(text, min_length=min_length, max_length=max_length)\n",
    "    return result[0][\"summary_text\"] if result else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Paths, sampling knobs, flags for embeddings/abstractive models, and dataset settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return normalize_whitespace(text).lower().split()\n",
    "\n",
    "def _ngram(tokens: List[str], n: int) -> List[tuple]:\n",
    "    return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def rouge_n(reference: str, candidate: str, n: int = 1) -> Dict[str, float]:\n",
    "    ref_tokens = _tokenize(reference)\n",
    "    cand_tokens = _tokenize(candidate)\n",
    "    if not ref_tokens or not cand_tokens:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    ref_ngrams = _ngram(ref_tokens, n)\n",
    "    cand_ngrams = _ngram(cand_tokens, n)\n",
    "    ref_counts: Dict[tuple, int] = {}\n",
    "    for ng in ref_ngrams:\n",
    "        ref_counts[ng] = ref_counts.get(ng, 0) + 1\n",
    "    overlap = 0\n",
    "    for ng in cand_ngrams:\n",
    "        if ref_counts.get(ng, 0) > 0:\n",
    "            overlap += 1\n",
    "            ref_counts[ng] -= 1\n",
    "    precision = overlap / max(len(cand_ngrams), 1)\n",
    "    recall = overlap / max(len(ref_ngrams), 1)\n",
    "    f1 = 0.0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def _lcs_length(a: List[str], b: List[str]) -> int:\n",
    "    # Classic dynamic-programming LCS for small sequences\n",
    "    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "    for i in range(1, len(a) + 1):\n",
    "        for j in range(1, len(b) + 1):\n",
    "            if a[i - 1] == b[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def rouge_l(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    ref_tokens = _tokenize(reference)\n",
    "    cand_tokens = _tokenize(candidate)\n",
    "    if not ref_tokens or not cand_tokens:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    lcs = _lcs_length(ref_tokens, cand_tokens)\n",
    "    precision = lcs / len(cand_tokens)\n",
    "    recall = lcs / len(ref_tokens)\n",
    "    f1 = 0.0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def embedding_cosine(\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    "    embedder: Optional[object] = None,\n",
    "    vectorizer: Optional[TfidfVectorizer] = None,\n",
    ") -> float:\n",
    "    if embedder is not None:\n",
    "        vecs = embedder.encode([reference, candidate], normalize_embeddings=True)\n",
    "        return float(np.dot(vecs[0], vecs[1]))\n",
    "    if vectorizer is not None:\n",
    "        mat = vectorizer.transform([reference, candidate])\n",
    "        sims = cosine_similarity(mat[0], mat[1])\n",
    "        return float(sims[0, 0])\n",
    "    return float(\"nan\")\n",
    "\n",
    "def evaluate_methods(\n",
    "    df: pd.DataFrame,\n",
    "    summarizers: Dict[str, Callable[[str], str]],\n",
    "    vectorizer: Optional[TfidfVectorizer],\n",
    "    embedder: Optional[object] = None,\n",
    "    max_eval: int = EVAL_ROWS,\n",
    ") -> pd.DataFrame:\n",
    "    sample = df.sample(min(max_eval, len(df)), random_state=RANDOM_STATE)\n",
    "    records = []\n",
    "    for _, row in sample.iterrows():\n",
    "        reference = row[\"summary\"]\n",
    "        for name, func in summarizers.items():\n",
    "            summary = func(reference)\n",
    "            r1 = rouge_n(reference, summary, n=1)\n",
    "            r2 = rouge_n(reference, summary, n=2)\n",
    "            rl = rouge_l(reference, summary)\n",
    "            sim = embedding_cosine(reference, summary, embedder=embedder, vectorizer=vectorizer)\n",
    "            records.append(\n",
    "                {\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"category\": row[\"category\"],\n",
    "                    \"method\": name,\n",
    "                    \"summary\": summary,\n",
    "                    \"rouge1_f1\": r1[\"f1\"],\n",
    "                    \"rouge2_f1\": r2[\"f1\"],\n",
    "                    \"rougeL_f1\": rl[\"f1\"],\n",
    "                    \"embedding_cosine\": sim,\n",
    "                    \"compression_ratio\": len(_tokenize(summary)) / max(len(_tokenize(reference)), 1),\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and summarization helpers\n",
    "Normalization, sentence splitting, data loader, extractive baselines (Lead-k, TF-IDF), TextRank, and abstractive helpers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_df = load_arxiv_dataset(\n",
    "    path=DATA_PATH,\n",
    "    max_rows=MAX_ROWS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    min_words=MIN_WORDS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    focus_prefixes=FOCUS_PREFIXES,\n",
    "    per_category_limit=PER_CATEGORY_LIMIT,\n",
    ")\n",
    "print(f\"Loaded {len(abstracts_df)} abstracts for experimentation\")\n",
    "abstracts_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation utilities\n",
    "Lightweight ROUGE-1/2/L, embedding cosine, and evaluator to score methods on a sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = fit_tfidf_vectorizer(abstracts_df[\"summary\"], max_features=8000)\n",
    "\n",
    "# Optional: enable sentence embeddings if sentence-transformers is installed and weights are local.\n",
    "embedder = None\n",
    "if USE_SENTENCE_EMBEDDINGS and SentenceTransformer is not None:\n",
    "    try:\n",
    "        # Model choice keeps memory/cpu needs low; change if you have the weights cached.\n",
    "        embedder = SentenceTransformer(str(EMBED_MODEL_PATH), local_files_only=True, device='cpu')\n",
    "    except Exception as exc:\n",
    "        print(f\"Embedding model unavailable: {exc}\")\n",
    "        embedder = None\n",
    "\n",
    "# Optional abstractive model: set USE_ABSTRACTIVE = False if transformers weights are present.\n",
    "abstractive_model = None\n",
    "if USE_ABSTRACTIVE:\n",
    "    abstractive_model = get_abstractive_summarizer(model_name=ABSTRACTIVE_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Reads arXiv abstracts with filtering/sampling and shows the head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizers = {\n",
    "    \"lead_1\": lambda text: lead_k_summary(text, k=1),\n",
    "    \"lead_2\": lambda text: lead_k_summary(text, k=2),\n",
    "    \"tfidf_top2\": lambda text: tfidf_extractive_summary(text, tfidf_vectorizer, max_sentences=2),\n",
    "    \"textrank_top2\": lambda text: textrank_extractive_summary(text, tfidf_vectorizer, max_sentences=2),\n",
    "}\n",
    "if abstractive_model is not None:\n",
    "    summarizers[\"abstractive_bart\"] = lambda text: abstractive_summary(text, abstractive_model)\n",
    "print(f\"Configured summarizers: {list(summarizers.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit vectorizer and load optional models\n",
    "Builds TF-IDF, then (optionally) loads local sentence-transformer embeddings and abstractive model from cached paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_methods(\n",
    "    df=abstracts_df,\n",
    "    summarizers=summarizers,\n",
    "    vectorizer=tfidf_vectorizer,\n",
    "    embedder=embedder,\n",
    "    max_eval=EVAL_ROWS,\n",
    ")\n",
    "\n",
    "print(results_df.head().to_string(index=False))\n",
    "print(\"\\nAverage scores by method:\")\n",
    "avg_tbl = (\n",
    "    results_df.groupby(\"method\")[['rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'embedding_cosine', 'compression_ratio']]\n",
    "    .mean()\n",
    "    .sort_values(by=\"rouge1_f1\", ascending=False)\n",
    ")\n",
    "print(avg_tbl.round(3).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "Bar and box plots for ROUGE and embedding scores across methods; images also saved to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg = results_df.groupby('method')[['rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'embedding_cosine']].mean().reset_index()\n",
    "\n",
    "# Bar plot of mean ROUGE-1 F1\n",
    "fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
    "ax1.bar(agg['method'], agg['rouge1_f1'], color='steelblue')\n",
    "ax1.set_ylabel('ROUGE-1 F1 (mean)')\n",
    "ax1.set_title('Mean ROUGE-1 by method')\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "fig1.savefig('plot_rouge1_bar.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Box plot of ROUGE-1 F1 distribution\n",
    "fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
    "results_df.boxplot(column='rouge1_f1', by='method', ax=ax2)\n",
    "ax2.set_title('ROUGE-1 F1 distribution by method')\n",
    "ax2.set_ylabel('ROUGE-1 F1')\n",
    "plt.suptitle('')\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "fig2.savefig('plot_rouge1_box.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('Saved plots: plot_rouge1_bar.png, plot_rouge1_box.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure summarizers\n",
    "Registers extractive baselines, TextRank, and optional abstractive summarizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist evaluation sample for inspection/reporting\n",
    "results_df.to_csv(RESULTS_PATH, index=False)\n",
    "print(f\"Saved evaluation sample to {RESULTS_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation sample\n",
    "Scores a sample of abstracts with each method and prints aggregate metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_category = results_df.groupby(['method', 'category'])[['rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'embedding_cosine', 'compression_ratio']].mean().reset_index()\n",
    "print(per_category.round(3).head().to_string(index=False))\n",
    "per_category.to_csv('eval_results_by_category.csv', index=False)\n",
    "print('Saved per-category means to eval_results_by_category.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
